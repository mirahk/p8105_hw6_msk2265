---
title: "p8105_hw6_msk2265"
author: "Mirah"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(modelr)
library(mgcv)
```


```{r, include=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())

```


The boostrap is helpful when you’d like to perform inference for a parameter / value / summary that doesn’t have an easy-to-write-down distribution in the usual repeated sampling framework. We’ll focus on a simple linear regression with tmax as the response with tmin and prcp as the predictors, and are interested in the distribution of two quantities estimated from these data:

r^2
log(β^1∗β^2)

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities.
```{r}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

```

```{r}
weather_df %>% 
  modelr::bootstrap(1000)
```


```{r}
#creating bootstrap samples
boot_df = weather_df %>% 
  bootstrap(5000) |> 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin + prcp, data = .)),
    results_tidy = map(models, broom::tidy),
    results_glance = map(models, broom::glance)) |> 
  select(-strap, -models) %>% 
  unnest(results_tidy) %>% 
  unnest(results_glance, names_repair = "minimal") %>% 
  select(-p.value, -statistic, -std.error) %>% 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>% 
  mutate(logbetas=log((prcp*tmin))
  )


```

Plot the distribution of your estimates, and describe these in words. Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r^2
 and log(β^0∗β^1)
```{r}
#plot distribution
boot_df %>% 
  ggplot(aes(x = r.squared)) +geom_density()
```

The distribution of r.squared looks to be negatively skewed

```{r}
boot_df %>% 
  ggplot(aes(x = logbetas)) +geom_density()


```
The distribution of logbetas is shows a stronger negative skew than the r.sqauared distribution 
```{r}
boot_df %>% 
  summarise(med=median(r.squared),
            lowerqt=quantile(r.squared, 0.025),
            upperqt=quantile(r.squared, 0.975))
```
quantiles of r.squared are in the table above
```{r}
boot_df %>% 
  summarise(med=median(logbetas, na.rm=TRUE),
            lowerqt=quantile(logbetas, 0.025, na.rm=TRUE),
            upperqt=quantile(logbetas, 0.975, na.rm=TRUE))
```

quantiles of logbetas are in the table above.


##Problem 3
```{r}
#tidy data
birth_weight_df = read_csv("./Data/birthweight.csv") %>% 
  mutate(
    babysex = 
      case_match(
        babysex,
        1 ~ "male",
        2 ~ "female"),
    frace =
      case_match(
        frace,
        1 ~ "white",
        2 ~ "black",
        3 ~ "asian",
        4 ~ "puerto rican",
        8 ~ "other"),
    mrace =
      case_match(
        mrace,
        1 ~ "white",
        2 ~ "black",
        3 ~ "asian",
        4 ~ "puerto rican",
        8 ~ "other"),
    malform =
      case_match(
        malform,
        0 ~ "absent",
        1 ~ "present")
      )
      
```

Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process and show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot.

The fist thing I evaluated when making my model, was several scatterplots to visualize the relationship between bwt and other numeric variables. The plots are below:
```{r}
#model1
#making plots to see if I can see relationships:

birth_weight_df %>% ggplot(aes(x=bhead, y=bwt))+geom_point()
#shows kinda linear
birth_weight_df %>% ggplot(aes(x=blength, y=bwt))+geom_point()
#shows kinda linear
birth_weight_df %>% ggplot(aes(x=delwt, y=bwt))+geom_point()
birth_weight_df %>% ggplot(aes(x=fincome, y=bwt))+geom_point()
birth_weight_df %>% ggplot(aes(x=gaweeks, y=bwt))+geom_point()
#shows kinda linear
birth_weight_df %>% ggplot(aes(x=menarche, y=bwt))+geom_point()
birth_weight_df %>% ggplot(aes(x=mheight, y=bwt))+geom_point()
birth_weight_df %>% ggplot(aes(x=momage, y=bwt))+geom_point()
birth_weight_df %>% ggplot(aes(x=ppbmi, y=bwt))+geom_point()
birth_weight_df %>% ggplot(aes(x=ppwt, y=bwt))+geom_point()
birth_weight_df %>% ggplot(aes(x=smoken, y=bwt))+geom_point()
birth_weight_df %>% ggplot(aes(x=wtgain, y=bwt))+geom_point()
```
 
Second, I compared the categorical variables by calculating averages. 

```{r}
birth_weight_df %>% 
  group_by(babysex) %>% 
  summarise(avwt=mean(bwt)) %>% 
  ungroup()
#not that different

birth_weight_df %>% 
  group_by(frace) %>% 
  summarise(avwt=mean(bwt)) %>% 
  ungroup()
#not that different
birth_weight_df %>% 
  group_by(mrace) %>% 
  summarise(avwt=mean(bwt)) %>% 
  ungroup()
#not that diffeent
birth_weight_df %>% 
  group_by(malform) %>% 
  summarise(avwt=mean(bwt)) %>% 
  ungroup()
#not that different
```
Based on all of those results, and the results of my scatter plots, I came up with the model below. I would like to note that hypothesis tests would have been a good idea for the categorical variables above to detwemine if there averages are statistically significantly different or not, and I would do that in a full report. I would also probably calculate correlation coefficiants for the scatterplots. 


```{r}

model1 = lm(bwt ~ bhead + gaweeks + blength, data = birth_weight_df)
summary(model1)

#plot residuals against fitted values

birth_weight_df %>% 
  add_residuals(model1) %>% 
  add_predictions(model1) %>% 
  ggplot(aes(x=resid, y=pred)) +geom_point()
```
All three of the predictors I chose are significant, based on the p values. The residuals versus fitted values plot indicates equal variance because the points hover around 0 for the resid, with some outliers that have a high resid. 
Compare your model to two others:

One using length at birth and gestational age as predictors (main effects only)
```{r}
model2 = lm(bwt ~ blength + gaweeks, data = birth_weight_df)
summary(model2)
```

One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
```{r}
model3 = lm(bwt ~ bhead + blength + babysex +bhead*blength +bhead*babysex +blength*babysex + bhead*blength*babysex , data = birth_weight_df)
summary(model3)
```

Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.

```{r}
cv_df = crossv_mc(birth_weight_df, 100)
cv_df |> pull(train) |> nth(1) |> as_tibble()
cv_df |> pull(test) |> nth(1) |> as_tibble()
cv_df =
 
   cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
cv_df = 
  cv_df |> 
  mutate(
    model1_tst  = map(train, ~model1),
    model2_tst  = map(train, ~model2),
    model3_tst  = map(train, ~model3))|> 
  mutate(
    rmse_model1 = map2_dbl(model1_tst, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_model2 = map2_dbl(model2_tst, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_model3 = map2_dbl(model3_tst, test, \(mod, df) rmse(model = mod, data = df)))

cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

Based on the violin graph, I can see that my model, and the 3rd model are the best fit, since the rmse is smallest.
